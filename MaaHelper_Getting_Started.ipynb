{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 MaaHelper v0.0.5 - Complete Getting Started Guide\n",
    "\n",
    "**Advanced AI-Powered Coding Assistant with Real-time Analysis & Git Integration**\n",
    "\n",
    "This notebook will guide you through all features of MaaHelper v0.0.5, from basic usage to advanced workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Table of Contents\n",
    "\n",
    "1. [Installation & Setup](#installation)\n",
    "2. [Basic Usage](#basic-usage)\n",
    "3. [Dynamic Model Discovery](#model-discovery)\n",
    "4. [Real-time Code Analysis](#code-analysis)\n",
    "5. [Smart Git Integration](#git-integration)\n",
    "6. [Vibecoding System](#vibecoding)\n",
    "7. [Advanced Configuration](#configuration)\n",
    "8. [Pro Tips & Workflows](#pro-tips)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📦 Installation & Setup {#installation}\n",
    "\n",
    "Let's start by installing MaaHelper and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MaaHelper v0.0.5\n",
    "!pip install maahelper\n",
    "\n",
    "# Or upgrade if you have an older version\n",
    "!pip install --upgrade maahelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import maahelper\n",
    "print(f\"✅ MaaHelper version: {maahelper.__version__}\")\n",
    "print(f\"📝 Description: {maahelper.__description__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔑 API Key Setup\n",
    "\n",
    "Set up your API keys for different AI providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set up API keys (choose the providers you want to use)\n",
    "print(\"🔑 Setting up API keys...\")\n",
    "print(\"You can skip any provider by pressing Enter\")\n",
    "\n",
    "# OpenAI (GPT models)\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    openai_key = getpass(\"Enter OpenAI API key (or press Enter to skip): \")\n",
    "    if openai_key:\n",
    "        os.environ['OPENAI_API_KEY'] = openai_key\n",
    "        print(\"✅ OpenAI API key set\")\n",
    "\n",
    "# Anthropic (Claude models)\n",
    "if not os.getenv('ANTHROPIC_API_KEY'):\n",
    "    anthropic_key = getpass(\"Enter Anthropic API key (or press Enter to skip): \")\n",
    "    if anthropic_key:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
    "        print(\"✅ Anthropic API key set\")\n",
    "\n",
    "# Groq (Fast inference)\n",
    "if not os.getenv('GROQ_API_KEY'):\n",
    "    groq_key = getpass(\"Enter Groq API key (or press Enter to skip): \")\n",
    "    if groq_key:\n",
    "        os.environ['GROQ_API_KEY'] = groq_key\n",
    "        print(\"✅ Groq API key set\")\n",
    "\n",
    "# Google (Gemini models)\n",
    "if not os.getenv('GOOGLE_API_KEY'):\n",
    "    google_key = getpass(\"Enter Google API key (or press Enter to skip): \")\n",
    "    if google_key:\n",
    "        os.environ['GOOGLE_API_KEY'] = google_key\n",
    "        print(\"✅ Google API key set\")\n",
    "\n",
    "print(\"\\n🎉 API key setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🎯 Basic Usage {#basic-usage}\n",
    "\n",
    "Let's start with basic MaaHelper functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core components\n",
    "from maahelper import (\n",
    "    create_llm_client, \n",
    "    get_all_providers, \n",
    "    get_provider_models,\n",
    "    config_manager\n",
    ")\n",
    "\n",
    "# Check available providers\n",
    "providers = get_all_providers()\n",
    "print(\"🤖 Available AI Providers:\")\n",
    "for provider in providers:\n",
    "    print(f\"  • {provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLM client (using OpenAI as example)\n",
    "# You can change this to any provider you have API keys for\n",
    "provider = \"openai\"  # Change to: \"anthropic\", \"groq\", or \"google\"\n",
    "model = \"gpt-4o-mini\"  # Fast and cost-effective model\n",
    "\n",
    "try:\n",
    "    llm_client = create_llm_client(\n",
    "        provider=provider,\n",
    "        model=model,\n",
    "        api_key=os.getenv(f\"{provider.upper()}_API_KEY\")\n",
    "    )\n",
    "    print(f\"✅ Created {provider} client with model {model}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating client: {e}\")\n",
    "    print(\"💡 Make sure you have set the API key for the chosen provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic chat functionality\n",
    "async def test_basic_chat():\n",
    "    if 'llm_client' in globals():\n",
    "        try:\n",
    "            response = await llm_client.achat_completion([\n",
    "                {\"role\": \"user\", \"content\": \"Hello! Explain what MaaHelper is in one sentence.\"}\n",
    "            ])\n",
    "            print(\"🤖 AI Response:\")\n",
    "            print(response)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "    else:\n",
    "        print(\"❌ No LLM client available. Please set up API keys first.\")\n",
    "\n",
    "# Run the test\n",
    "import asyncio\n",
    "await test_basic_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 🔍 Dynamic Model Discovery {#model-discovery}\n",
    "\n",
    "MaaHelper v0.0.5 can automatically discover the latest models from all providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model discovery\n",
    "from maahelper.features.model_discovery import model_discovery\n",
    "\n",
    "# Discover all available models\n",
    "async def discover_models():\n",
    "    print(\"🔍 Discovering available models...\")\n",
    "    \n",
    "    try:\n",
    "        # Get models for all providers\n",
    "        all_models = await model_discovery.get_available_models(force_refresh=True)\n",
    "        \n",
    "        print(\"\\n✨ Model Discovery Results:\")\n",
    "        for provider, models in all_models.items():\n",
    "            print(f\"\\n🤖 {provider.upper()}: {len(models)} models\")\n",
    "            \n",
    "            # Show first 5 models with details\n",
    "            for model in models[:5]:\n",
    "                context_info = f\" ({model.context_length:,} tokens)\" if model.context_length > 0 else \"\"\n",
    "                streaming = \"📡\" if model.supports_streaming else \"❌\"\n",
    "                functions = \"🔧\" if model.supports_function_calling else \"❌\"\n",
    "                print(f\"  • {model.id}{context_info} {streaming} {functions}\")\n",
    "            \n",
    "            if len(models) > 5:\n",
    "                print(f\"  ... and {len(models) - 5} more models\")\n",
    "        \n",
    "        print(\"\\n📡 = Streaming support | 🔧 = Function calling support\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error discovering models: {e}\")\n",
    "\n",
    "await discover_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get models by capability\n",
    "async def models_by_capability():\n",
    "    print(\"🎯 Models by Capability:\")\n",
    "    \n",
    "    # Get models with long context (>32k tokens)\n",
    "    long_context_models = model_discovery.list_models_by_capability(\"long_context\")\n",
    "    print(f\"\\n📚 Long Context Models ({len(long_context_models)}):\")\n",
    "    for model in long_context_models[:3]:\n",
    "        print(f\"  • {model.provider}: {model.id} ({model.context_length:,} tokens)\")\n",
    "    \n",
    "    # Get models with function calling\n",
    "    function_models = model_discovery.list_models_by_capability(\"function_calling\")\n",
    "    print(f\"\\n🔧 Function Calling Models ({len(function_models)}):\")\n",
    "    for model in function_models[:3]:\n",
    "        print(f\"  • {model.provider}: {model.id}\")\n",
    "    \n",
    "    # Get streaming models\n",
    "    streaming_models = model_discovery.list_models_by_capability(\"streaming\")\n",
    "    print(f\"\\n📡 Streaming Models ({len(streaming_models)}):\")\n",
    "    for model in streaming_models[:3]:\n",
    "        print(f\"  • {model.provider}: {model.id}\")\n",
    "\n",
    "await models_by_capability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 📊 Real-time Code Analysis {#code-analysis}\n",
    "\n",
    "MaaHelper can analyze your code in real-time and provide suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample Python file for analysis\n",
    "sample_code = '''\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def calculate_sum(a, b):\n",
    "    \"\"\"Calculate sum of two numbers\"\"\"\n",
    "    result = a + b\n",
    "    return result\n",
    "\n",
    "def process_data(data):\n",
    "    # This function is too long and has issues\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    processed = []\n",
    "    for item in data:\n",
    "        if item > 0:\n",
    "            processed.append(item * 2)\n",
    "        else:\n",
    "            processed.append(0)\n",
    "    \n",
    "    # Inefficient string concatenation\n",
    "    result_string = \"\"\n",
    "    for item in processed:\n",
    "        result_string += str(item) + \",\"\n",
    "    \n",
    "    return result_string\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def process(self, data):\n",
    "        return process_data(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = DataProcessor(\"test\")\n",
    "    result = processor.process([1, 2, 3, -1, 0])\n",
    "    print(result)\n",
    "'''\n",
    "\n",
    "# Write sample code to file\n",
    "with open(\"sample_code.py\", \"w\") as f:\n",
    "    f.write(sample_code)\n",
    "\n",
    "print(\"✅ Created sample_code.py for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import real-time analyzer\n",
    "from maahelper.features.realtime_analysis import realtime_analyzer, CodeAnalyzer\n",
    "from pathlib import Path\n",
    "\n",
    "# Analyze the sample file\n",
    "async def analyze_sample_code():\n",
    "    print(\"📊 Analyzing sample code...\")\n",
    "    \n",
    "    analyzer = CodeAnalyzer()\n",
    "    result = await analyzer.analyze_file(Path(\"sample_code.py\"))\n",
    "    \n",
    "    print(f\"\\n📄 File: {result.file_path}\")\n",
    "    print(f\"📈 Metrics: {result.metrics}\")\n",
    "    \n",
    "    if result.issues:\n",
    "        print(f\"\\n🔍 Found {len(result.issues)} issues:\")\n",
    "        \n",
    "        # Group issues by severity\n",
    "        by_severity = {}\n",
    "        for issue in result.issues:\n",
    "            if issue.severity not in by_severity:\n",
    "                by_severity[issue.severity] = []\n",
    "            by_severity[issue.severity].append(issue)\n",
    "        \n",
    "        # Display issues\n",
    "        severity_icons = {\n",
    "            \"error\": \"❌\",\n",
    "            \"warning\": \"⚠️\",\n",
    "            \"suggestion\": \"💡\",\n",
    "            \"info\": \"ℹ️\"\n",
    "        }\n",
    "        \n",
    "        for severity, issues in by_severity.items():\n",
    "            icon = severity_icons.get(severity, \"•\")\n",
    "            print(f\"\\n{icon} {severity.upper()} ({len(issues)}):\")\n",
    "            \n",
    "            for issue in issues[:3]:  # Show first 3 of each type\n",
    "                print(f\"  Line {issue.line_number}: {issue.message}\")\n",
    "                if issue.suggestion:\n",
    "                    print(f\"    💡 {issue.suggestion}\")\n",
    "    else:\n",
    "        print(\"✅ No issues found!\")\n",
    "\n",
    "await analyze_sample_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entire workspace\n",
    "async def analyze_workspace():\n",
    "    print(\"🔍 Analyzing entire workspace...\")\n",
    "    \n",
    "    # Set workspace to current directory\n",
    "    realtime_analyzer.workspace_path = Path(\".\")\n",
    "    \n",
    "    # Analyze all files\n",
    "    results = await realtime_analyzer.analyze_workspace()\n",
    "    \n",
    "    # Get summary\n",
    "    summary = realtime_analyzer.get_summary()\n",
    "    \n",
    "    print(f\"\\n📊 Workspace Analysis Summary:\")\n",
    "    print(f\"  📁 Files analyzed: {summary.get('files_analyzed', 0)}\")\n",
    "    print(f\"  🔍 Total issues: {summary.get('total_issues', 0)}\")\n",
    "    print(f\"  ❌ Errors: {summary.get('errors', 0)}\")\n",
    "    print(f\"  ⚠️ Warnings: {summary.get('warnings', 0)}\")\n",
    "    print(f\"  💡 Suggestions: {summary.get('suggestions', 0)}\")\n",
    "    print(f\"  📄 Files with errors: {summary.get('files_with_errors', 0)}\")\n",
    "\n",
    "await analyze_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 🔧 Smart Git Integration {#git-integration}\n",
    "\n",
    "MaaHelper can help with Git operations using AI-generated commit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Git integration\n",
    "from maahelper.features.git_integration import git_integration, GitAnalyzer\n",
    "\n",
    "# Check if we're in a Git repository\n",
    "import subprocess\n",
    "\n",
    "def check_git_repo():\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'status'], capture_output=True, text=True)\n",
    "        return result.returncode == 0\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "is_git_repo = check_git_repo()\n",
    "print(f\"📁 Git repository: {'✅ Yes' if is_git_repo else '❌ No'}\")\n",
    "\n",
    "if not is_git_repo:\n",
    "    print(\"💡 To test Git features, initialize a Git repository:\")\n",
    "    print(\"   git init\")\n",
    "    print(\"   git add .\")\n",
    "    print(\"   git commit -m 'Initial commit'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Git changes (if in a Git repo)\n",
    "async def analyze_git_changes():\n",
    "    if not is_git_repo:\n",
    "        print(\"❌ Not in a Git repository\")\n",
    "        return\n",
    "    \n",
    "    print(\"🔍 Analyzing Git changes...\")\n",
    "    \n",
    "    analyzer = GitAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        # Get current Git status\n",
    "        changes = await analyzer.get_git_status()\n",
    "        \n",
    "        if not changes:\n",
    "            print(\"✅ No changes detected\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📋 Found {len(changes)} changes:\")\n",
    "        for change in changes:\n",
    "            change_icon = {\n",
    "                \"added\": \"➕\",\n",
    "                \"modified\": \"📝\",\n",
    "                \"deleted\": \"❌\",\n",
    "                \"renamed\": \"🔄\"\n",
    "            }.get(change.change_type, \"•\")\n",
    "            \n",
    "            change_info = \"\"\n",
    "            if change.lines_added or change.lines_removed:\n",
    "                change_info = f\" (+{change.lines_added} -{change.lines_removed})\"\n",
    "            \n",
    "            print(f\"  {change_icon} {change.file_path}{change_info}\")\n",
    "        \n",
    "        # Generate commit message suggestion\n",
    "        if 'llm_client' in globals():\n",
    "            print(\"\\n🤖 Generating commit message...\")\n",
    "            suggestion = await analyzer.generate_commit_message(changes, llm_client)\n",
    "            \n",
    "            print(f\"\\n💡 Suggested commit message:\")\n",
    "            print(f\"   {suggestion.type}({suggestion.scope}): {suggestion.description}\")\n",
    "            if suggestion.body:\n",
    "                print(f\"\\n   {suggestion.body}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing Git changes: {e}\")\n",
    "\n",
    "await analyze_git_changes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate branch name suggestion\n",
    "async def suggest_branch_name():\n",
    "    analyzer = GitAnalyzer()\n",
    "    \n",
    "    # Example descriptions\n",
    "    descriptions = [\n",
    "        \"Add user authentication system\",\n",
    "        \"Fix memory leak in data processor\",\n",
    "        \"Implement real-time notifications\",\n",
    "        \"Update documentation for API endpoints\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🌿 Branch name suggestions:\")\n",
    "    for desc in descriptions:\n",
    "        branch_name = await analyzer.suggest_branch_name(desc)\n",
    "        print(f\"  '{desc}' → {branch_name}\")\n",
    "\n",
    "await suggest_branch_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 🎨 Vibecoding System {#vibecoding}\n",
    "\n",
    "The vibecoding system provides specialized AI prompts for different coding scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vibecoding components\n",
    "from maahelper.vibecoding.prompts import vibecoding_prompts\n",
    "from maahelper.vibecoding.commands import vibecoding_commands\n",
    "from maahelper.vibecoding.workflow import vibecoding_workflow\n",
    "\n",
    "# Set up vibecoding with LLM client\n",
    "if 'llm_client' in globals():\n",
    "    vibecoding_commands.llm_client = llm_client\n",
    "    vibecoding_workflow.llm_client = llm_client\n",
    "    print(\"✅ Vibecoding system ready with AI client\")\n",
    "else:\n",
    "    print(\"⚠️ Vibecoding system ready (no AI client - will use fallback responses)\")\n",
    "\n",
    "# List available prompts\n",
    "print(\"\\n🎨 Available Vibecoding Prompts:\")\n",
    "categories = vibecoding_prompts.get_categories()\n",
    "for category in categories:\n",
    "    prompts = vibecoding_prompts.list_prompts(category)\n",
    "    print(f\"\\n📂 {category.upper()}:\")\n",
    "    for prompt in prompts:\n",
    "        print(f\"  • {prompt.name}: {prompt.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Code Review\n",
    "async def vibecoding_code_review():\n",
    "    print(\"🔍 Vibecoding: Code Review Example\")\n",
    "    \n",
    "    # Read our sample code\n",
    "    with open(\"sample_code.py\", \"r\") as f:\n",
    "        code_to_review = f.read()\n",
    "    \n",
    "    # Perform AI code review\n",
    "    review_result = await vibecoding_commands.code_review(\n",
    "        code=code_to_review,\n",
    "        language=\"python\",\n",
    "        context=\"Sample data processing script\",\n",
    "        focus_areas=\"performance, maintainability, best practices\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n📝 Code Review Result:\")\n",
    "    print(review_result[:500] + \"...\" if len(review_result) > 500 else review_result)\n",
    "\n",
    "await vibecoding_code_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Bug Analysis\n",
    "async def vibecoding_bug_analysis():\n",
    "    print(\"🐛 Vibecoding: Bug Analysis Example\")\n",
    "    \n",
    "    buggy_code = '''\n",
    "def divide_numbers(a, b):\n",
    "    return a / b\n",
    "\n",
    "def process_list(items):\n",
    "    result = []\n",
    "    for i in range(len(items) + 1):  # Bug: off-by-one error\n",
    "        result.append(items[i] * 2)\n",
    "    return result\n",
    "'''\n",
    "    \n",
    "    # Analyze the bug\n",
    "    bug_analysis = await vibecoding_commands.bug_analysis(\n",
    "        problem=\"Function crashes with IndexError\",\n",
    "        code=buggy_code,\n",
    "        language=\"python\",\n",
    "        error_details=\"IndexError: list index out of range\",\n",
    "        environment=\"Python 3.9\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🔧 Bug Analysis Result:\")\n",
    "    print(bug_analysis[:500] + \"...\" if len(bug_analysis) > 500 else bug_analysis)\n",
    "\n",
    "await vibecoding_bug_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Workflow Management\n",
    "async def vibecoding_workflow_demo():\n",
    "    print(\"🔄 Vibecoding: Workflow Management Demo\")\n",
    "    \n",
    "    # Start a coding session\n",
    "    session_result = await vibecoding_workflow.start_coding_session(\n",
    "        project_description=\"Data Analysis Dashboard\",\n",
    "        goals=[\"Create data visualization\", \"Add user authentication\", \"Optimize performance\"]\n",
    "    )\n",
    "    print(f\"\\n{session_result}\")\n",
    "    \n",
    "    # Analyze project structure\n",
    "    project_analysis = await vibecoding_workflow.analyze_project_structure()\n",
    "    print(f\"\\n📊 Project Analysis:\")\n",
    "    print(f\"  Files: {len(project_analysis['files'])}\")\n",
    "    print(f\"  Languages: {', '.join(project_analysis['languages'])}\")\n",
    "    print(f\"  Has tests: {project_analysis['structure']['has_tests']}\")\n",
    "    \n",
    "    # Get next task suggestion\n",
    "    next_task = await vibecoding_workflow.suggest_next_task(\n",
    "        current_context=\"Just started the project, need to set up basic structure\"\n",
    "    )\n",
    "    print(f\"\\n💡 Next Task Suggestion:\")\n",
    "    print(next_task[:300] + \"...\" if len(next_task) > 300 else next_task)\n",
    "    \n",
    "    # Get session summary\n",
    "    summary = vibecoding_workflow.get_session_summary()\n",
    "    print(f\"\\n📋 Session Summary:\")\n",
    "    print(summary[:300] + \"...\" if len(summary) > 300 else summary)\n",
    "\n",
    "await vibecoding_workflow_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ⚙️ Advanced Configuration {#configuration}\n",
    "\n",
    "MaaHelper v0.0.5 includes a comprehensive configuration system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration manager\n",
    "from maahelper.config.config_manager import config_manager, AppConfig\n",
    "\n",
    "# Load current configuration\n",
    "config = config_manager.load_config()\n",
    "\n",
    "print(\"⚙️ Current Configuration:\")\n",
    "print(f\"  App Name: {config.app_name}\")\n",
    "print(f\"  Version: {config.version}\")\n",
    "print(f\"  Debug Mode: {config.debug}\")\n",
    "print(f\"  Log Level: {config.log_level}\")\n",
    "print(f\"  Config Directory: {config.config_dir}\")\n",
    "print(f\"  Workspace Path: {config.workspace_path}\")\n",
    "\n",
    "print(f\"\\n🎨 UI Configuration:\")\n",
    "print(f\"  Theme: {config.ui.theme}\")\n",
    "print(f\"  Show Timestamps: {config.ui.show_timestamps}\")\n",
    "print(f\"  Show Token Count: {config.ui.show_token_count}\")\n",
    "\n",
    "print(f\"\\n🚀 Performance Configuration:\")\n",
    "print(f\"  Max Concurrent Requests: {config.performance.max_concurrent_requests}\")\n",
    "print(f\"  Request Timeout: {config.performance.request_timeout}s\")\n",
    "print(f\"  Retry Attempts: {config.performance.retry_attempts}\")\n",
    "print(f\"  Cache Responses: {config.performance.cache_responses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available LLM providers configuration\n",
    "print(\"🤖 LLM Providers Configuration:\")\n",
    "for provider_name, provider_config in config.llm_providers.items():\n",
    "    print(f\"\\n📡 {provider_name.upper()}:\")\n",
    "    print(f\"  Base URL: {provider_config.base_url}\")\n",
    "    print(f\"  Default Model: {provider_config.default_model}\")\n",
    "    print(f\"  Max Tokens: {provider_config.max_tokens}\")\n",
    "    print(f\"  Temperature: {provider_config.temperature}\")\n",
    "    print(f\"  Models: {len(provider_config.models)} available\")\n",
    "    \n",
    "    # Show first few models\n",
    "    if provider_config.models:\n",
    "        print(f\"  Sample Models: {', '.join(provider_config.models[:3])}\")\n",
    "        if len(provider_config.models) > 3:\n",
    "            print(f\"    ... and {len(provider_config.models) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate configuration\n",
    "validation_issues = config_manager.validate_config()\n",
    "\n",
    "if validation_issues:\n",
    "    print(\"⚠️ Configuration Issues Found:\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"  • {issue}\")\n",
    "else:\n",
    "    print(\"✅ Configuration is valid!\")\n",
    "\n",
    "# Show how to customize configuration\n",
    "print(\"\\n🔧 To customize configuration:\")\n",
    "print(\"1. Edit ~/.maahelper/config.yaml\")\n",
    "print(\"2. Set environment variables:\")\n",
    "print(\"   export MAAHELPER_DEBUG=true\")\n",
    "print(\"   export MAAHELPER_LOG_LEVEL=DEBUG\")\n",
    "print(\"3. Use config_manager.save_config() to persist changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 🎯 Pro Tips & Advanced Workflows {#pro-tips}\n",
    "\n",
    "Advanced techniques for power users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro Tip 1: Rate Limiting and Performance\n",
    "from maahelper.utils.rate_limiter import global_rate_limiter\n",
    "\n",
    "print(\"🚀 Pro Tip 1: Rate Limiting & Performance\")\n",
    "print(\"\\nMaaHelper includes intelligent rate limiting:\")\n",
    "\n",
    "# Get rate limiting stats\n",
    "stats = global_rate_limiter.get_stats()\n",
    "if stats:\n",
    "    print(\"\\n📊 Current Rate Limiting Stats:\")\n",
    "    for key, stat in stats.items():\n",
    "        print(f\"  {key}:\")\n",
    "        print(f\"    Requests (last minute): {stat['requests_last_minute']}\")\n",
    "        print(f\"    Can make request: {stat['can_make_request']}\")\n",
    "        if stat['wait_time'] > 0:\n",
    "            print(f\"    Wait time: {stat['wait_time']:.1f}s\")\n",
    "else:\n",
    "    print(\"  No rate limiting data available yet\")\n",
    "\n",
    "print(\"\\n💡 Tips:\")\n",
    "print(\"  • Rate limiting prevents API quota exhaustion\")\n",
    "print(\"  • Automatic retry with exponential backoff\")\n",
    "print(\"  • Burst protection for sudden request spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro Tip 2: Memory Management\n",
    "from maahelper.utils.memory_manager import memory_manager\n",
    "\n",
    "print(\"🧠 Pro Tip 2: Memory Management\")\n",
    "\n",
    "# Create a sample conversation session\n",
    "session = memory_manager.create_session(\n",
    "    session_id=\"demo_session\",\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# Add some messages\n",
    "memory_manager.add_message(\"demo_session\", \"user\", \"Hello, how are you?\", tokens=10)\n",
    "memory_manager.add_message(\"demo_session\", \"assistant\", \"I'm doing well, thank you!\", tokens=15)\n",
    "memory_manager.add_message(\"demo_session\", \"user\", \"Can you help me with Python?\", tokens=12)\n",
    "\n",
    "# Get session stats\n",
    "stats = memory_manager.get_session_stats(\"demo_session\")\n",
    "if stats:\n",
    "    print(f\"\\n📊 Session Stats:\")\n",
    "    print(f\"  Messages: {stats['message_count']}\")\n",
    "    print(f\"  Total Tokens: {stats['total_tokens']}\")\n",
    "    print(f\"  Age: {stats['age_hours']:.2f} hours\")\n",
    "    print(f\"  Provider: {stats['provider']}\")\n",
    "    print(f\"  Model: {stats['model']}\")\n",
    "\n",
    "# Get global memory stats\n",
    "global_stats = memory_manager.get_global_stats()\n",
    "print(f\"\\n🌍 Global Memory Stats:\")\n",
    "print(f\"  Total Sessions: {global_stats['total_sessions']}\")\n",
    "print(f\"  Total Messages: {global_stats['total_messages']}\")\n",
    "print(f\"  Total Tokens: {global_stats['total_tokens']}\")\n",
    "\n",
    "print(\"\\n💡 Memory Management Features:\")\n",
    "print(\"  • Automatic cleanup of old conversations\")\n",
    "print(\"  • Token-based memory limits\")\n",
    "print(\"  • Persistent storage support\")\n",
    "print(\"  • Session-based organization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro Tip 3: Input Validation & Security\n",
    "from maahelper.utils.input_validator import input_validator\n",
    "\n",
    "print(\"🔒 Pro Tip 3: Input Validation & Security\")\n",
    "\n",
    "# Test various input validations\n",
    "test_cases = [\n",
    "    (\"string\", \"Hello, world!\", {}),\n",
    "    (\"file_path\", \"../../../etc/passwd\", {\"must_exist\": False}),\n",
    "    (\"file_path\", \"sample_code.py\", {\"must_exist\": True}),\n",
    "    (\"url\", \"https://api.openai.com/v1/models\", {}),\n",
    "    (\"api_key\", \"sk-1234567890abcdef\", {}),\n",
    "    (\"integer\", \"42\", {\"min_val\": 0, \"max_val\": 100}),\n",
    "    (\"float\", \"3.14159\", {\"min_val\": 0.0, \"max_val\": 10.0})\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 Validation Test Results:\")\n",
    "for validation_type, test_value, kwargs in test_cases:\n",
    "    if validation_type == \"string\":\n",
    "        result = input_validator.validate_string(test_value, **kwargs)\n",
    "    elif validation_type == \"file_path\":\n",
    "        result = input_validator.validate_file_path(test_value, **kwargs)\n",
    "    elif validation_type == \"url\":\n",
    "        result = input_validator.validate_url(test_value, **kwargs)\n",
    "    elif validation_type == \"api_key\":\n",
    "        result = input_validator.validate_api_key(test_value, **kwargs)\n",
    "    elif validation_type == \"integer\":\n",
    "        result = input_validator.validate_integer(test_value, **kwargs)\n",
    "    elif validation_type == \"float\":\n",
    "        result = input_validator.validate_float(test_value, **kwargs)\n",
    "    \n",
    "    status = \"✅\" if result.is_valid else \"❌\"\n",
    "    print(f\"  {status} {validation_type}: '{test_value}'\")\n",
    "    \n",
    "    if result.errors:\n",
    "        for error in result.errors:\n",
    "            print(f\"    ❌ {error}\")\n",
    "    \n",
    "    if result.warnings:\n",
    "        for warning in result.warnings:\n",
    "            print(f\"    ⚠️ {warning}\")\n",
    "\n",
    "print(\"\\n🛡️ Security Features:\")\n",
    "print(\"  • Path traversal protection\")\n",
    "print(\"  • Input sanitization\")\n",
    "print(\"  • Dangerous pattern detection\")\n",
    "print(\"  • File extension validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro Tip 4: Logging System\n",
    "from maahelper.utils.logging_system import get_logger, configure_logging\n",
    "\n",
    "print(\"📝 Pro Tip 4: Structured Logging\")\n",
    "\n",
    "# Configure logging\n",
    "configure_logging(\n",
    "    level=\"INFO\",\n",
    "    format=\"structured\",\n",
    "    console_enabled=True\n",
    ")\n",
    "\n",
    "# Get a logger\n",
    "logger = get_logger(\"demo\")\n",
    "\n",
    "# Test different log levels\n",
    "print(\"\\n📊 Logging Examples:\")\n",
    "logger.info(\"Application started\", component=\"demo\", version=\"0.0.5\")\n",
    "logger.warning(\"This is a warning message\", category=\"test\")\n",
    "logger.error(\"This is an error message\", error_code=\"E001\")\n",
    "\n",
    "# Log performance metrics\n",
    "logger.log_performance(\"model_inference\", 0.245, tokens=150, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Log API request\n",
    "logger.log_request(\"openai\", \"gpt-4o-mini\", 150, 0.245, status=\"success\")\n",
    "\n",
    "print(\"\\n📋 Logging Features:\")\n",
    "print(\"  • Structured logging with Rich output\")\n",
    "print(\"  • Configurable log levels\")\n",
    "print(\"  • File rotation support\")\n",
    "print(\"  • Performance metrics logging\")\n",
    "print(\"  • API request tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro Tip 5: Complete Development Workflow\n",
    "print(\"🔄 Pro Tip 5: Complete Development Workflow\")\n",
    "print(\"\\nHere's how to use MaaHelper for a complete development workflow:\")\n",
    "\n",
    "workflow_steps = [\n",
    "    \"1. 🔍 Start with model discovery: `discover-models`\",\n",
    "    \"2. 📊 Enable real-time analysis: `analyze-start`\",\n",
    "    \"3. 🎨 Start vibecoding session for your project\",\n",
    "    \"4. 💻 Write code with live feedback\",\n",
    "    \"5. 🔧 Use vibecoding commands for specific tasks\",\n",
    "    \"6. 🔍 Review code with AI: `vibe-review`\",\n",
    "    \"7. 🐛 Debug issues with AI: `vibe-debug`\",\n",
    "    \"8. 🚀 Optimize performance: `vibe-optimize`\",\n",
    "    \"9. 📝 Smart Git commit: `git-commit`\",\n",
    "    \"10. 🌿 Create feature branches: `git-branch`\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(\"\\n🎯 Best Practices:\")\n",
    "best_practices = [\n",
    "    \"• Set up API keys for multiple providers for redundancy\",\n",
    "    \"• Use real-time analysis during development\",\n",
    "    \"• Leverage vibecoding for complex tasks\",\n",
    "    \"• Use smart Git integration for better commit messages\",\n",
    "    \"• Configure logging for debugging and monitoring\",\n",
    "    \"• Regularly update models with dynamic discovery\",\n",
    "    \"• Use memory management for long conversations\",\n",
    "    \"• Validate inputs for security and reliability\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"  {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Conclusion\n",
    "\n",
    "Congratulations! You've completed the MaaHelper v0.0.5 getting started guide.\n",
    "\n",
    "### 🌟 What You've Learned\n",
    "\n",
    "- ✅ **Installation & Setup**: API keys and basic configuration\n",
    "- ✅ **Dynamic Model Discovery**: Auto-fetch latest AI models\n",
    "- ✅ **Real-time Code Analysis**: Live error detection and suggestions\n",
    "- ✅ **Smart Git Integration**: AI-powered commit messages and branch names\n",
    "- ✅ **Vibecoding System**: Specialized AI prompts and workflows\n",
    "- ✅ **Advanced Configuration**: Customization and optimization\n",
    "- ✅ **Pro Tips**: Rate limiting, memory management, security, and logging\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Start the CLI**: Run `maahelper` in your terminal\n",
    "2. **Try New Commands**: Use `discover-models`, `analyze-start`, `git-commit`\n",
    "3. **Explore Vibecoding**: Use specialized commands for your projects\n",
    "4. **Customize Configuration**: Edit `~/.maahelper/config.yaml`\n",
    "5. **Join the Community**: Share feedback and contribute\n",
    "\n",
    "### 📚 Resources\n",
    "\n",
    "- **Documentation**: [README.md](README.md)\n",
    "- **Changelog**: [CHANGELOG.md](CHANGELOG.md)\n",
    "- **Features**: [FEATURES_v0.0.5.md](FEATURES_v0.0.5.md)\n",
    "- **GitHub**: [https://github.com/AIMLDev726/maahelper](https://github.com/AIMLDev726/maahelper)\n",
    "\n",
    "---\n",
    "\n",
    "**🌟 Happy coding with MaaHelper v0.0.5!**\n",
    "\n",
    "*Your AI-powered development companion is ready to supercharge your workflow.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove sample files created during this tutorial\n",
    "import os\n",
    "\n",
    "files_to_cleanup = [\"sample_code.py\"]\n",
    "\n",
    "print(\"🧹 Cleaning up tutorial files...\")\n",
    "for file in files_to_cleanup:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"  ✅ Removed {file}\")\n",
    "\n",
    "print(\"\\n🎉 Tutorial complete! MaaHelper v0.0.5 is ready for your projects.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
